"""
This workflow prepares the data needed for the rest of the analysis:

* Downloads reads from ENA for pf6, pvgv and pacb_ilmn_pf. Multiple ENA samples/runs are concatenated.
  The reads are subsampled to a maximum genome coverage and also trimmed.

* Downloads the PACB assemblies for pacb_ilmn_pf.

* Downloads the falciparum and vivax reference genomes (and gffs).

"""

from pathlib import Path

WORKFLOW = "download_data"


configfile: "analysis/configs/common.yaml"
configfile: f"analysis/configs/{WORKFLOW}.yaml"


include: "../common_utils.py"
include: f"../{WORKFLOW}/utils.py"


output_base = Path(f'{config["output_dir"]}/{WORKFLOW}')
output_gene_beds = Path(f"{output_base}/gene_beds")
output_ref_genomes = Path(f"{output_base}/ref_genomes")
output_pacb_pf_assemblies = Path(f"{output_base}/pacb_ilmn_pf/assemblies")
output_vcfs = Path(f"{output_base}/vcfs")
mk_output_dirs(dir())

ena_records = (
    load_pf6(config["pf6_tsv"], ignored_pattern="ENA")
    + load_pvgv(config["pvgv_tsv"])
    + load_pacb_ilmn_pf(config["pacb_ilmn_pf_tsv"])
)

sample_to_ena_IDs = {record.sample_name: record.ena_IDs for record in ena_records}
pacb_pf_fnames = [
    f"{record.sample_name}.April2018.fasta.gz"
    for record in load_pacb_ilmn_pf(config["pacb_ilmn_pf_tsv"])
]

pf_chrom_list = list(map(lambda v: f"0{v}", range(1,10))) + list(map(str,range(10,15)))


rule all:
    input:
        ena_reads=expand(
            f"{output_base}/{{record.dataset_name}}/{{record.sample_name}}/reads_{{i}}.final.fastq.gz",
            record=ena_records,
            i=[1, 2],
        ),
        pacb_pf_assemblies=expand(
            f"{output_pacb_pf_assemblies}/{{assembly_fname}}",
            assembly_fname=pacb_pf_fnames,
        ),
        ref_genomes=expand(
            f"{output_ref_genomes}/{{genome}}{{extension}}",
            genome=config["ref_genomes"]["genomes"],
            extension=config["ref_genomes"]["extensions"],
        ),
        gene_beds=expand(
            f"{output_gene_beds}/{{gene_list_name}}.bed",
            gene_list_name=config["gene_list_names"],
        ),
        vcfs=f"{output_vcfs}/pf6/combined.vcf.gz",


rule download_data_ena:
    output:
        expand(
            "{output_base}/{{dataset_name}}/{{sample_name}}/reads_{i}.fastq.gz",
            output_base=output_base,
            i=[1, 2],
        ),
    params:
        script=f'{config["scripts"]}/{WORKFLOW}/dl_ena.py',
        ena_IDs=lambda wildcards: sample_to_ena_IDs[wildcards.sample_name],
    shell:
        f"python3 {{params.script}} {{params.ena_IDs}} {output_base}/{{wildcards.dataset_name}}/{{wildcards.sample_name}}"


rule download_data_subsample_reads:
    input:
        ir1=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_1.fastq.gz",
        ir2=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_2.fastq.gz",
    output:
        or1=temp(
            f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_1.subsampled.fastq.gz"
        ),
        or2=temp(
            f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_2.subsampled.fastq.gz"
        ),
    params:
        genome_size=lambda wildcards: get_genome_size(wildcards.dataset_name),
        max_coverage=config["subsampling_maxcov"],
    container:
        config["container_gramtools"]
    resources:
        mem_mb=4000,
    shell:
        "rasusa -i {input.ir1} -i {input.ir2} --coverage {params.max_coverage} --genome-size {params.genome_size} -o {output.or1} -o {output.or2}"


rule download_data_trim_reads:
    input:
        ir1=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_1.subsampled.fastq.gz",
        ir2=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_2.subsampled.fastq.gz",
    output:
        or1=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_1.final.fastq.gz",
        or2=f"{output_base}/{{dataset_name}}/{{sample_name}}/reads_2.final.fastq.gz",
    threads: 4
    container:
        config["container_gramtools"]
    resources:
        mem_mb=10000,
    shell:
        "java -jar /software/Trimmomatic-0.36/trimmomatic-0.36.jar PE -threads {threads} -phred33 {input.ir1} {input.ir2} {output.or1} /dev/null {output.or2} /dev/null ILLUMINACLIP:/software/Trimmomatic-0.36/adapters/TruSeq3-PE-2.fa:2:30:10 LEADING:10 TRAILING:10 SLIDINGWINDOW:4:15 MINLEN:50"


rule download_data_pacb_pf_assemblies:
    output:
        rules.all.input.pacb_pf_assemblies,
    shell:
        f"wget {config['pacb_pf_ftp']} -nc -P {output_pacb_pf_assemblies}"


rule download_data_ref_genomes:
    wildcard_constraints:
        extension="\.(genome.fasta.gz|gff3.gz)",
    output:
        f"{output_ref_genomes}/{{genome}}{{extension}}",
    shell:
        f"wget {config['ref_genomes']['ftp_root']}/{{wildcards.genome}}{{wildcards.extension}} -P {output_ref_genomes}"


rule download_data_index_ref_genomes:
    input:
        f"{output_ref_genomes}/{{genome}}.fasta.gz",
    output:
        f"{output_ref_genomes}/{{genome}}.fasta.gz.fai",
    container:
        config["container_gramtools"]
    shell:
        """
        input={input}
        to_gzip="${{input/.gz/}}"
        # The vivax genome has n's, and gramtools does not deal with n's yet
        fastaq replace_bases {input} "$to_gzip" n c
        bgzip --force "$to_gzip"
        samtools faidx {input}
        """


rule download_data_make_gene_bed:
    input:
        gene_list_file=f'{config["gene_list_dir"]}/{{gene_list_name}}.txt',
        genome_gff=(
            lambda wildcards: f"{output_ref_genomes}/{gene_list_to_genome(wildcards)}.gff3.gz"
        ),
    output:
        gene_bed=f"{output_gene_beds}/{{gene_list_name}}.bed",
    container:
        config["container_gramtools"]
    params:
        gff_to_bed_script=f'{config["scripts"]}/{WORKFLOW}/gff_to_bed.sh',
    shell:
        "bash {params.gff_to_bed_script} {input.gene_list_file} {input.genome_gff} {output}"

ruleorder: download_data_concat_vcfs > download_data_get_vcf

rule download_data_get_vcf:
    output:
        vcf=f"{output_vcfs}/{{dataset_name}}/{{chrom}}.vcf.gz",
    params:
        vcf_url=get_vcf_url,
    shell:
        """
        mkdir -p $(dirname {output.vcf})
        if [[ ! -e {output.vcf} ]]; then
            wget {params.vcf_url} -nc -O {output.vcf}
        fi
        wget {params.vcf_url}.md5 -O {output.vcf}.md5
        # Check downloaded file integrity
        diff <(cut -f 1 -d ' ' {output.vcf}.md5) <(md5sum {output.vcf} | cut -f 1 -d ' ')
        """


rule download_data_concat_vcfs:
    input:
        vcfs=expand(
                f"{output_vcfs}/{{dataset_name}}/{{chrom}}.vcf.gz",
                chrom=pf_chrom_list,
                allow_missing=True
        )
    output:
        vcf=f"{output_vcfs}/{{dataset_name}}/combined.vcf.gz",
    shell:
        """
        bcftools concat {input.vcfs} > {output.vcf}
        """
